{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f3cf64",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bedcaba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aes\\anaconda3\\envs\\SeniorDesignProject\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import re\n",
    "import torch\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aedd77",
   "metadata": {},
   "source": [
    "# Visual Inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2250b581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set minimum confidence threshold:  0.4\n",
      "Enter target classes separated by commas (or leave blank for all):  elephant, car, person\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\Aes\\Desktop\\Telit_test\\images\\000000356125.jpg: 480x640 2 persons, 1 elephant, 21.8ms\n",
      "Speed: 2.7ms preprocess, 21.8ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1moutputs\\annotated\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\Aes\\Desktop\\Telit_test\\images\\000000356169.jpg: 480x640 3 cars, 17.8ms\n",
      "Speed: 1.8ms preprocess, 17.8ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1moutputs\\annotated2\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\Aes\\Desktop\\Telit_test\\images\\000000356248.jpg: 640x480 4 persons, 22.0ms\n",
      "Speed: 1.7ms preprocess, 22.0ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1moutputs\\annotated3\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\Aes\\Desktop\\Telit_test\\images\\000000356261.jpg: 480x640 (no detections), 16.5ms\n",
      "Speed: 1.9ms preprocess, 16.5ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1moutputs\\annotated4\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\Aes\\Desktop\\Telit_test\\images\\000000376206.jpg: 448x640 1 person, 20.6ms\n",
      "Speed: 1.7ms preprocess, 20.6ms inference, 6.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1moutputs\\annotated5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set minimum confidence threshold:  0.4\n",
      "Enter target classes separated by commas (or leave blank for all):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\Aes\\Desktop\\Telit_test\\images\\000000356125.jpg: 480x640 2 persons, 1 elephant, 26.6ms\n",
      "Speed: 2.4ms preprocess, 26.6ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1moutputs\\annotated6\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\Aes\\Desktop\\Telit_test\\images\\000000356169.jpg: 480x640 3 cars, 27.3ms\n",
      "Speed: 63.5ms preprocess, 27.3ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1moutputs\\annotated7\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\Aes\\Desktop\\Telit_test\\images\\000000356248.jpg: 640x480 4 persons, 1 bottle, 2 cups, 1 chair, 1 potted plant, 25.8ms\n",
      "Speed: 1.3ms preprocess, 25.8ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1moutputs\\annotated8\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\Aes\\Desktop\\Telit_test\\images\\000000356261.jpg: 480x640 1 horse, 14.4ms\n",
      "Speed: 1.2ms preprocess, 14.4ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1moutputs\\annotated9\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\Aes\\Desktop\\Telit_test\\images\\000000376206.jpg: 448x640 1 person, 15.8ms\n",
      "Speed: 2.0ms preprocess, 15.8ms inference, 3.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1moutputs\\annotated10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#path configurations\n",
    "INPUT_DIR = Path(\"images/\")\n",
    "OUTPUT_DIR = Path(\"outputs/\")\n",
    "CROPS_DIR = OUTPUT_DIR / \"crops\"\n",
    "\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CROPS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOCATIONS = [\"zoo\", \"street\", \"restaurant\", \"river\", \"beach\"]\n",
    "\n",
    "def detect_objects():\n",
    "    #load model\n",
    "    model = YOLO(\"yolov8n-seg.pt\")\n",
    "    results = []\n",
    "    \n",
    "    threshold = float(input(\"\\nSet minimum confidence threshold: \"))\n",
    "    \n",
    "    #allow for user input to be either a percentage or a decimal\n",
    "    if threshold > 1.0:\n",
    "        threshold = threshold / 100.0\n",
    "    \n",
    "    classes_input = input(\"Enter target classes separated by commas (or leave blank for all): \")\n",
    "    target_classes = [c.strip() for c in classes_input.split(\",\")] if classes_input != \"\" else []\n",
    "    \n",
    "    \n",
    "    for idx, img_path in enumerate(INPUT_DIR.glob(\"*.*\")):\n",
    "        #yolo model inference\n",
    "        outputs = model.predict(\n",
    "            source=str(img_path),\n",
    "            conf=float(threshold),\n",
    "            save=True,\n",
    "            save_txt=False,\n",
    "            project=str(OUTPUT_DIR),\n",
    "            name=\"annotated\",\n",
    "            classes=[k for k, v in model.names.items() if v in target_classes] if target_classes else None\n",
    "        )\n",
    "        \n",
    "        img = cv2.imread(str(img_path))\n",
    "    \n",
    "    \n",
    "        #random timestamp within the last 7 days (for the agentic section)\n",
    "        now = datetime.now()            \n",
    "        random_seconds = random.randint(0, 7 * 24 * 60 * 60)\n",
    "        random_timestamp = now - timedelta(seconds=random_seconds)\n",
    "        \n",
    "        for output in outputs:\n",
    "            if output.masks is not None:\n",
    "                masks = output.masks.data.cpu().numpy()\n",
    "            else:\n",
    "                masks = []\n",
    "    \n",
    "    \n",
    "            for i, box in enumerate(output.boxes):\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                cls_id = int(box.cls[0])\n",
    "                cls_name = model.names[cls_id]\n",
    "                conf = float(box.conf[0])\n",
    "                \n",
    "                #save crop\n",
    "                crop_dir = CROPS_DIR / cls_name\n",
    "                crop_dir.mkdir(parents=True, exist_ok=True)\n",
    "                crop_img = img[y1:y2, x1:x2]\n",
    "                crop_fname = f\"{img_path.stem}_{cls_name}_{x1}_{y1}_{x2}_{y2}.jpg\"\n",
    "                cv2.imwrite(str(crop_dir / crop_fname), crop_img)\n",
    "    \n",
    "    \n",
    "                if len(masks) > i:\n",
    "                    mask = (masks[i] * 255).astype(np.uint8)  # convert to 0-255\n",
    "                    mask_path = OUTPUT_DIR / f\"{img_path.stem}_{cls_name}_mask.png\"\n",
    "                    cv2.imwrite(str(mask_path), mask)\n",
    "    \n",
    "    \n",
    "                \n",
    "                results.append({\n",
    "                    \"image\": img_path.name,\n",
    "                    \"class\": cls_name,\n",
    "                    \"confidence\": conf,\n",
    "                    \"bbox\": [x1, y1, x2, y2],\n",
    "    \n",
    "                    #fake metadata for agentic section\n",
    "                    \"camera\": f\"CAM{idx}\", \n",
    "                    \"timestamp\": str(random_timestamp),\n",
    "                    \"location\": LOCATIONS[idx],\n",
    "                })\n",
    "    \n",
    "    \n",
    "    with open(OUTPUT_DIR / \"detections.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "detect_objects()\n",
    "detect_objects()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44659f",
   "metadata": {},
   "source": [
    "# Agentic AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "268e4ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: [[{'image': '000000356169.jpg', 'class': 'car', 'confidence': 0.5915552377700806, 'bbox': [365, 323, 381, 336], 'camera': 'CAM1', 'timestamp': '2025-08-12 05:23:30.777668', 'location': 'street'}, {'image': '000000356169.jpg', 'class': 'car', 'confidence': 0.4703880548477173, 'bbox': [332, 323, 357, 337], 'camera': 'CAM1', 'timestamp': '2025-08-12 05:23:30.777668', 'location': 'street'}, {'image': '000000356169.jpg', 'class': 'car', 'confidence': 0.45832645893096924, 'bbox': [0, 295, 56, 357], 'camera': 'CAM1', 'timestamp': '2025-08-12 05:23:30.777668', 'location': 'street'}], \"Summary for {'camera': None, 'date': None, 'location': 'zoo'}: 3 notable detections.\"]\n",
      "\n",
      "\n",
      "Assistant: Statistics for {'camera': 'CAM4', 'date': None, 'location': None}: 1 detections, 1 unique classes.\n"
     ]
    }
   ],
   "source": [
    "YOLO_JSON_PATH = Path(\"outputs/detections.json\")\n",
    "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "INTENT_LABELS = [\"fetch_latest_detections\", \"summarize_events\", \"report_statistics\"]\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "CAMERAS = [f\"CAM{i}\" for i in range(1, 6)]\n",
    "LOCATIONS = [\"zoo\", \"street\", \"restaurant\", \"river\", \"beach\"]\n",
    "\n",
    "#extract parameters from user request\n",
    "def extract_parameters(text):\n",
    "    params = {\"camera\": None, \"date\": None, \"location\": None}\n",
    "\n",
    "    for cam in CAMERAS:\n",
    "        if cam.lower() in text.lower():\n",
    "            params[\"camera\"] = cam\n",
    "\n",
    "    for loc in LOCATIONS:\n",
    "        if loc.lower() in text.lower():\n",
    "            params[\"location\"] = loc\n",
    "\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            datetime.strptime(word, \"%Y-%m-%d\")\n",
    "            params[\"date\"] = word\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return params\n",
    "\n",
    "#classify intents into \"fetch_latest_detections\", \"summarize_events\", or \"report_statistics\"\n",
    "def classify_request(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        intent_id = torch.argmax(logits, dim=-1).item()\n",
    "        intent = INTENT_LABELS[intent_id % len(INTENT_LABELS)]\n",
    "    return intent\n",
    "\n",
    "#filter documents based on user request\n",
    "def filter_documents(documents, params):\n",
    "    filtered = documents\n",
    "    if params.get(\"date\"):\n",
    "        filtered = [d for d in filtered if d[\"date\"].startswith(params[\"date\"])]\n",
    "    if params.get(\"camera\"):\n",
    "        filtered = [d for d in filtered if str(d[\"camera\"]) == params[\"camera\"]]\n",
    "    if params.get(\"location\"):\n",
    "        filtered = [d for d in filtered if params[\"location\"].lower() in d[\"location\"].lower()]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def fetch_latest_detections(params):\n",
    "    if not YOLO_JSON_PATH.exists():\n",
    "        return \"No detection data found.\"\n",
    "\n",
    "    with open(YOLO_JSON_PATH) as f:\n",
    "        detections = json.load(f)\n",
    "\n",
    "    filtered = filter_documents(detections, params)\n",
    "    return filtered\n",
    "\n",
    "def summarize_events(params):\n",
    "    with open(YOLO_JSON_PATH) as f:\n",
    "        detections = json.load(f)\n",
    "\n",
    "    filtered = filter_documents(detections, params)    \n",
    "    return f\"Summary for {params or 'all data'}: {len(filtered)} notable detections.\"\n",
    "\n",
    "\n",
    "def report_statistics(params):\n",
    "    with open(YOLO_JSON_PATH) as f:\n",
    "        detections = json.load(f)\n",
    "        \n",
    "    filtered = filter_documents(detections, params)\n",
    "\n",
    "    total_detections = len(filtered)\n",
    "    unique_classes = len(set(d[\"class\"] for d in filtered))\n",
    "    return f\"Statistics for {params or 'all data'}: {total_detections} detections, {unique_classes} unique classes.\"\n",
    "\n",
    "\n",
    "#handle request, then run the appropriate actions\n",
    "def handle_request(user_input):\n",
    "\n",
    "    #the embedding model has trouble classifying requests as \"report_statistics\", so it is handled separately here. \n",
    "    if any(word in user_input.lower() for word in [\"stats\", \"statistics\", \"report\"]):\n",
    "        intent = \"report_statistics\"\n",
    "    else:\n",
    "        intent = classify_request(user_input)\n",
    "    params = extract_parameters(user_input)\n",
    "    \n",
    "\n",
    "    #multi-part request handling\n",
    "    if \" and \" in user_input.lower():\n",
    "        parts = [p.strip() for p in user_input.split(\" and \")]\n",
    "        responses = []\n",
    "        for p in parts:\n",
    "            sub_intent = classify_request(p)\n",
    "            sub_params = extract_parameters(p)\n",
    "            responses.append(run_action(sub_intent, sub_params))\n",
    "        return responses\n",
    "    else:\n",
    "        return run_action(intent, params)\n",
    "\n",
    "#run the action based on the intent, return the appropriate result\n",
    "def run_action(intent, params):\n",
    "    try:\n",
    "        with open(\"requests.json\", \"r\") as f:\n",
    "            history = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        history = []\n",
    "\n",
    "    \n",
    "\n",
    "    if intent == \"fetch_latest_detections\":\n",
    "        response =  fetch_latest_detections(params)\n",
    "    elif intent == \"summarize_events\":\n",
    "        response = summarize_events(params)\n",
    "    elif intent == \"report_statistics\":\n",
    "        response =  report_statistics(params)\n",
    "    else:\n",
    "        response = \"Sorry, I didn't understand your request.\"\n",
    "\n",
    "    entry = {\n",
    "        \"user\": {\n",
    "            \"request type\": intent,\n",
    "            \"parameters\": params,\n",
    "        },\n",
    "        \"Assistant\": response\n",
    "    }\n",
    "\n",
    "    history.append(entry)\n",
    "    with open(\"requests.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    return response\n",
    "\n",
    "response = handle_request(\"Fetch latest detections from CAM1 and summarize events from the zoo.\")\n",
    "print(f\"Assistant: {response}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "response = handle_request(\"Report statistics for CAM4.\")\n",
    "print(f\"Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c29904",
   "metadata": {},
   "source": [
    "# YOLO + Agent Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4b1efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved detections to video_detections.json\n",
      "Camera 5 detected a person with confidence 0.43 at 2025-08-15 17:28:04.003175 in outside.\n",
      "Camera 5 detected a car with confidence 0.89 at 2025-08-15 17:27:58.185019 in outside.\n"
     ]
    }
   ],
   "source": [
    "VIDEO_PATH = \"20250815_131659.mp4\" \n",
    "OUTPUT_JSON = \"video_detections.json\"\n",
    "CONF_THRES = 0.3\n",
    "MODEL_PATH = \"yolov8n-seg.pt\"\n",
    "CLASSES = [\"elephant\", \"person\", \"car\", \"horse\", \"bottle\", \"cup\", \"chair\", \"potted plant\"] #these classes can be found in the dataset\n",
    "LOCATIONS = [\"zoo\", \"street\", \"restaurant\", \"river\", \"beach\", \"outside\"]\n",
    "\n",
    "\n",
    "#run YOLO on video and save detections to JSON\n",
    "def detect_objects():\n",
    "    model = YOLO(MODEL_PATH)\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "    frame_idx = 0\n",
    "    results = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        output = model.predict(source=frame, conf=CONF_THRES, verbose=False)\n",
    "\n",
    "        for o in output:\n",
    "            for i, box in enumerate(o.boxes):\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                cls_id = int(box.cls[0])\n",
    "                cls_name = model.names[cls_id]\n",
    "                conf = float(box.conf[0])\n",
    "\n",
    "\n",
    "                mask_points = None\n",
    "                if o.masks is not None:\n",
    "                    mask_points = o.masks.xy[i].tolist()  # polygon points\n",
    "\n",
    "                #fake metadata\n",
    "                start = datetime.now()\n",
    "                curr_time = start + timedelta(seconds=frame_idx / 60) #assuming 60 FPS video\n",
    "\n",
    "\n",
    "                results.append({\n",
    "                    \"frame\": frame_idx,\n",
    "                    \"class\": cls_name,\n",
    "                    \"confidence\": conf,\n",
    "                    \"bbox\": [x1, y1, x2, y2],\n",
    "                    \"timestamp\": str(curr_time),\n",
    "                    \"camera\": 5,\n",
    "                    \"location\": \"outside\"\n",
    "                })\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    with open(OUTPUT_JSON, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"[INFO] Saved detections to {OUTPUT_JSON}\")\n",
    "\n",
    "\n",
    "def add_embeddings(documents, embed_model):\n",
    "    for doc in documents:\n",
    "        doc[\"embedding\"] = embed_model.encode(doc[\"text\"], convert_to_numpy=True)\n",
    "\n",
    "def build_documents(detections, embed_model):\n",
    "    docs = []\n",
    "    for d in detections:\n",
    "        text = f\"Camera {d['camera']} detected a {d['class']} with confidence {d['confidence']:.2f} at {d['timestamp']} in {d['location']}.\"\n",
    "        docs.append({\n",
    "            \"text\": text,\n",
    "            \"metadata\": d\n",
    "        })\n",
    "    add_embeddings(docs, embed_model)\n",
    "    return docs\n",
    "\n",
    "def query_rag(question, documents, embed_model, k=5):\n",
    "    #extract filters\n",
    "    date, camera, location, obj_class = None, None, None, None\n",
    "\n",
    "    #date\n",
    "    for word in question.split():\n",
    "        try:\n",
    "            date = datetime.strptime(word, \"%Y-%m-%d\").date()\n",
    "            break\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    #camera\n",
    "    cam_match = re.search(r'CAM(\\d+)', question, re.IGNORECASE)\n",
    "    if cam_match:\n",
    "        camera = int(cam_match.group(1))\n",
    "\n",
    "    #location\n",
    "    for loc in LOCATIONS:\n",
    "        if loc.lower() in question.lower():\n",
    "            location = loc\n",
    "            break\n",
    "\n",
    "    #object class\n",
    "    for c in CLASSES:\n",
    "        if c.lower() in question.lower():\n",
    "            obj_class = c\n",
    "            break\n",
    "\n",
    "    #filter documents\n",
    "    filtered_docs = documents\n",
    "    if date:\n",
    "        filtered_docs = [d for d in filtered_docs\n",
    "                         if datetime.fromisoformat(d[\"metadata\"][\"timestamp\"]).date() == date]\n",
    "    if camera:\n",
    "        filtered_docs = [d for d in filtered_docs if d[\"metadata\"][\"camera\"] == camera]\n",
    "    if location:\n",
    "        filtered_docs = [d for d in filtered_docs if d[\"metadata\"][\"location\"].lower() == location.lower()]\n",
    "    if obj_class:\n",
    "        filtered_docs = [d for d in filtered_docs if d[\"metadata\"][\"class\"].lower() == obj_class.lower()]\n",
    "\n",
    "    if not filtered_docs:\n",
    "        return \"No detections found for your query.\"\n",
    "\n",
    "    #FAISS search\n",
    "    emb_matrix = np.stack([d[\"embedding\"] for d in filtered_docs])\n",
    "    q_emb = embed_model.encode(question, convert_to_numpy=True)\n",
    "    index = faiss.IndexFlatL2(emb_matrix.shape[1])\n",
    "    index.add(emb_matrix)\n",
    "    distances, indices = index.search(np.array([q_emb]), k)\n",
    "\n",
    "    retrieved_docs = [filtered_docs[i][\"text\"] for i in indices[0] if i < len(filtered_docs)]\n",
    "    return retrieved_docs[0]  #return top result\n",
    "\n",
    "\n",
    "detect_objects()\n",
    "\n",
    "with open(OUTPUT_JSON) as f:\n",
    "    detections = json.load(f)\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "documents = build_documents(detections, embed_model)\n",
    "print(query_rag(\"Which camera saw the most persons yesterday?\", documents, embed_model))\n",
    "print(query_rag(\"List timestamps where a car was detected.\", documents, embed_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
